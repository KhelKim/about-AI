# Transformer

1. multi-head self-attention
2. residual connection
3. positional embedding

